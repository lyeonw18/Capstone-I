# Capstone I
Ethically-Aware Reinforcement Learning for Fair Recidivism Prediction
## 1. Overview
This project was conducted as part of the undergraduate capstone course.
The goal of this project was to analyze the bias issues of the COMPAS algorithm and suggest ways to improve the model to strengthen the fairness and ethical responsibility of AI-based decision-making systems.
### Research Question:
Can reinforcement learning optimize predictive accuracy while maintaining demographic and individual fairness in high-stakes decision systems?

## 2. Dataset
- Source: ProPublica COMPAS dataset
- Task: Binary classification (2-year recidivism prediction)
- Sensitive attributes: Race, Age
- Key features:
    - Prior convictions
    - Charge degree
    - Age
    - Decile score
    - Juvenile records
### Preprocessing
  - Filtered COMPAS dataset based on screening date and validity conditions
  - Defined “current offense” per individual using temporal logic
  - Encoded categorical variables (Label Encoding)
  - Balanced violent vs non-violent samples using downsampling
  - Train/Test split (70/30)

## 3. Baseline Models
To evaluate performance trade-offs, we first trained conventional ML models:
  - Logistic Regression
  - Random Forest (GridSearchCV tuning)
  - XGBoost (Hyperparameter tuning)
  - SVM
  - Linear Regression

### Evaluation metric:
  - RMSE
  - Accuracy
  - Balanced Accuracy

## 4.  Proposed Ethical RL Framework Framework Architecture
   - Deep Q-Network (DQN)
   - Experience Replay Buffer
   - Target Network stabilization
   - Epsilon-greedy exploration
### Ethical Reward Design
  Designed a custom reward function integrating three ethical dimensions:
  1. **Group Fairness**
   - Demographic Parity penalty
   - Equalized Odds constraint

  2. **Responsibility**
   - Penalization of disproportionate false positives for vulnerable groups

  3. **Individual Fairness**
   - k-NN-based similarity consistency penalty

  Final integrated reward:

 ! Integrated Reward = (Fairness + Responsibility + Consistency) / 3

## 5. Comparative Experiments
compared our framework against three fairness-aware approaches under identical preprocessing and dataset conditions.
 1. **Fairness Constraints: Mechanisms for Fair Classification**
   - Integrates demographic parity as mathematical constraints
   - Applied linear and nonlinear constraints during training
   - Optimizes fairness during model fitting

  2. **Fairness Through Awareness**
   - Introduces the concept of Individual Fairness
   - “Similar individuals should be treated similarly”
   - Enforces prediction consistency based on defined distance metrics

  3. **Reinforcement Learning Framework for Fair Classification**
   - Reformulates classification as sequential decision-making
   - Uses reward-based optimization for fairness metrics
   - Supports Demographic Parity & Equal Opportunity


## 6. Results
### Accuracy
- All models showed comparable performance
- Our model maintained top-tier predictive accuracy

### Demographic Parity Difference (DPP)
- Competitive performance across models
- Stable fairness-accuracy trade-off

### Individual Fairness
- Fairness Through Awareness achieved strongest individual fairness
- Our model maintained balance between group & individual fairness

### Key Insight
Our framework achieved balanced optimization but revealed that multi-objective fairness reinforcement requires more sophisticated reward calibration.

## 7. Discussion: Decile Score Limitation
One notable limitation was the decile score variable:
 - It is internally generated by the COMPAS algorithm
 - Scoring mechanism lacks transparency
 - howed weak performance in DPP experiments
However:
- Cox proportional hazard analysis showed statistical significance
- Heatmap correlation analysis showed no multicollinearity
Therefore, instead of removing the feature, we conclude that future research should incorporate fairness-aware feature refinement.

## 8. Key Contributions
- Proposed an ethically-aware DQN framework for recidivism prediction
- Integrated Demographic Parity & Individual Fairness directly into reward
- Conducted structured comparison with fairness-aware research
- Demonstrated fairness–accuracy trade-off analysis
- Provided empirical discussion on sensitive internal scoring variables

## 9. What I Learned
- Fairness optimization is inherently multi-objective and non-trivial
- Reward design critically affects ethical outcomes in RL
- Balancing group fairness and individual fairness requires careful calibration
- Interpretability remains essential in high-stakes AI systems


## 10. Repository Structure



